{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging DQN_9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN_9 was the best performing model (on the F1-score) saved by Optuna optimisation. However during validation, the results are poor. This notebook troubleshoots that by re-training the model again with the best parameters. Turns out the problem is the best performing model wasn't saved correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import exists\n",
    "\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "from vimms.Evaluation import EvaluationData\n",
    "from vimms_gym.env import DDAEnv\n",
    "from vimms_gym.common import EVAL_METRIC_REWARD\n",
    "from vimms_gym.evaluation import evaluate\n",
    "\n",
    "from experiments import preset_qcb_small\n",
    "from tune import TrialEvalCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some useful methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_run(fname, max_peaks, params, n_eval_episodes=1, deterministic=True):\n",
    "    \n",
    "    custom_objects = {\n",
    "        \"learning_rate\": 0.0,\n",
    "        \"lr_schedule\": lambda _: 0.0,\n",
    "        \"clip_range\": lambda _: 0.0,\n",
    "    }    \n",
    "\n",
    "    model = DQN.load(fname, custom_objects=custom_objects)\n",
    "    eval_env = DDAEnv(max_peaks, params)\n",
    "    print(eval_env.env_params)\n",
    "    \n",
    "    # wrap env in Monitor, create the trial callback\n",
    "    eval_env = Monitor(eval_env)\n",
    "    eval_metric = EVAL_METRIC_REWARD\n",
    "    eval_callback = TrialEvalCallback(eval_env, None, eval_metric)\n",
    "    env = eval_callback.eval_env\n",
    "\n",
    "    assert eval_callback.deterministic == True\n",
    "    \n",
    "    # actual evaluation starts here\n",
    "    episode_count = 0\n",
    "    episode_count_target = n_eval_episodes\n",
    "    current_reward = 0\n",
    "    current_length = 0\n",
    "    observations = env.reset()\n",
    "    states = None\n",
    "    episode_starts = np.ones((env.num_envs,), dtype=bool)\n",
    "    episode_starts\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_eval_results = []\n",
    "    episode_lengths = []\n",
    "    start = timer()\n",
    "    while episode_count < episode_count_target:\n",
    "        actions, states = model.predict(observations, state=states,\n",
    "                                        episode_start=episode_starts,\n",
    "                                        deterministic=deterministic)\n",
    "        observations, rewards, dones, infos = env.step(actions)\n",
    "        # print(rewards, current_reward, current_length, dones)\n",
    "        episode_starts = dones\n",
    "        current_reward += rewards[0]\n",
    "        current_length += 1\n",
    "\n",
    "        if dones[0]:  # when done, episode would be reset automatically\n",
    "            val = current_reward\n",
    "            eval_res = evaluate(eval_data, format_output=False)\n",
    "            episode_eval_results.append(eval_res)\n",
    "            end = timer()\n",
    "            print('Evaluation episode %d finished: metric %f, timedelta=%s' % (\n",
    "                episode_count, val, str(timedelta(seconds=end - start))))\n",
    "            start = timer()\n",
    "            episode_rewards.append(val)\n",
    "            episode_lengths.append(current_length)\n",
    "            episode_count += 1\n",
    "            current_reward = 0\n",
    "            current_length = 0\n",
    "\n",
    "        # store previous results for evaluation before 'done'\n",
    "        # this needs to be here, because VecEnv is automatically reset when done\n",
    "        inner_env = env.envs[0].env\n",
    "        eval_data = EvaluationData(inner_env.vimms_env)\n",
    "    \n",
    "    return episode_rewards, episode_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_res_to_df(rewards, eval_res):\n",
    "    reward_mean = np.mean(rewards)\n",
    "    reward_std = np.std(rewards)\n",
    "\n",
    "    metric = [e['coverage_prop'] for e in eval_res]\n",
    "    coverage_mean = np.mean(metric)\n",
    "    coverage_std = np.std(metric)\n",
    "    \n",
    "    metric = [e['intensity_prop'] for e in eval_res]\n",
    "    intensity_prop_mean = np.mean(metric)\n",
    "    intensity_prop_std = np.std(metric)\n",
    "    \n",
    "    metric = [e['f1'] for e in eval_res]\n",
    "    f1_mean = np.mean(metric)\n",
    "    f1_std = np.std(metric)\n",
    "    \n",
    "    results = []\n",
    "    results.append(['reward', reward_mean, reward_std])\n",
    "    results.append(['coverage_prop', coverage_mean, coverage_std])\n",
    "    results.append(['intensity_prop', intensity_prop_mean, intensity_prop_std])\n",
    "    results.append(['f1', f1_mean, f1_std])    \n",
    "    df = pd.DataFrame(results, columns=['metric', 'mean', 'std'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate environment preset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 11:41:06.267 | INFO     | experiments:get_samplers:283 - Loaded /Users/joewandy/Work/git/vimms-gym/pickles/samplers_QCB_small_gaussian.p\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'chemical_creator': {'mz_range': (100, 110),\n",
       "   'rt_range': (400, 500),\n",
       "   'intensity_range': (10000.0, 1e+20),\n",
       "   'n_chemicals': (20, 50),\n",
       "   'mz_sampler': <vimms.ChemicalSamplers.MZMLFormulaSampler at 0x7fa7cfb06700>,\n",
       "   'ri_sampler': <vimms.ChemicalSamplers.MZMLRTandIntensitySampler at 0x7fa7cfb06370>,\n",
       "   'cr_sampler': <vimms.ChemicalSamplers.GaussianChromatogramSampler at 0x7fa7cfb19dc0>},\n",
       "  'noise': {'enable_spike_noise': True,\n",
       "   'noise_density': 0.1,\n",
       "   'noise_max_val': 1000.0,\n",
       "   'mz_range': (100, 110)},\n",
       "  'env': {'ionisation_mode': 'Positive',\n",
       "   'rt_range': (400, 500),\n",
       "   'isolation_window': 0.7,\n",
       "   'mz_tol': 10,\n",
       "   'rt_tol': 120,\n",
       "   'alpha': 0.191500954,\n",
       "   'beta': 0.030798858}},\n",
       " 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.191500954\n",
    "beta = 0.030798858\n",
    "extract = False\n",
    "params, max_peaks = preset_qcb_small(None, alpha=alpha, beta=beta, extract_chromatograms=extract)\n",
    "params, max_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_eval_episodes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load best saved model from Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join('..', 'DQN', 'DQN_9.zip') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vimms-gym/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:561: UserWarning:\n",
      "\n",
      "This system does not have apparently enough memory to store the complete replay buffer 9.87GB > 0.35GB\n",
      "\n",
      "/opt/anaconda3/envs/vimms-gym/lib/python3.9/site-packages/stable_baselines3/dqn/dqn.py:152: UserWarning:\n",
      "\n",
      "The number of environments used is greater than the target network update interval (20 > 1), therefore the target network will be updated after each call to env.step() which corresponds to 20 steps.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ionisation_mode': 'Positive', 'rt_range': (400, 500), 'isolation_window': 0.7, 'mz_tol': 10, 'rt_tol': 120, 'alpha': 0.191500954, 'beta': 0.030798858}\n",
      "Evaluation episode 0 finished: metric -499.000000, timedelta=0:00:00.322370\n",
      "Evaluation episode 1 finished: metric -499.000000, timedelta=0:00:00.316084\n",
      "Evaluation episode 2 finished: metric -499.000000, timedelta=0:00:00.315718\n",
      "Evaluation episode 3 finished: metric -499.000000, timedelta=0:00:00.305773\n",
      "Evaluation episode 4 finished: metric -499.000000, timedelta=0:00:00.374677\n",
      "Evaluation episode 5 finished: metric -499.000000, timedelta=0:00:00.323484\n",
      "Evaluation episode 6 finished: metric -499.000000, timedelta=0:00:00.326402\n",
      "Evaluation episode 7 finished: metric -499.000000, timedelta=0:00:00.308955\n",
      "Evaluation episode 8 finished: metric -499.000000, timedelta=0:00:00.311999\n",
      "Evaluation episode 9 finished: metric -499.000000, timedelta=0:00:00.314115\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reward</td>\n",
       "      <td>-499.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coverage_prop</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intensity_prop</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric   mean  std\n",
       "0          reward -499.0  0.0\n",
       "1   coverage_prop    0.0  0.0\n",
       "2  intensity_prop    0.0  0.0\n",
       "3              f1    0.0  0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards, eval_res = debug_run(fname, max_peaks, params, n_eval_episodes=n_eval_episodes, deterministic=True)\n",
    "df = eval_res_to_df(rewards, eval_res)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ionisation_mode': 'Positive', 'rt_range': (400, 500), 'isolation_window': 0.7, 'mz_tol': 10, 'rt_tol': 120, 'alpha': 0.191500954, 'beta': 0.030798858}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vimms-gym/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:561: UserWarning:\n",
      "\n",
      "This system does not have apparently enough memory to store the complete replay buffer 9.87GB > 0.35GB\n",
      "\n",
      "/opt/anaconda3/envs/vimms-gym/lib/python3.9/site-packages/stable_baselines3/dqn/dqn.py:152: UserWarning:\n",
      "\n",
      "The number of environments used is greater than the target network update interval (20 > 1), therefore the target network will be updated after each call to env.step() which corresponds to 20 steps.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation episode 0 finished: metric -471.907695, timedelta=0:00:00.305524\n",
      "Evaluation episode 1 finished: metric -469.009215, timedelta=0:00:00.317895\n",
      "Evaluation episode 2 finished: metric -460.838806, timedelta=0:00:00.474006\n",
      "Evaluation episode 3 finished: metric -481.639023, timedelta=0:00:00.243609\n",
      "Evaluation episode 4 finished: metric -474.898733, timedelta=0:00:00.396137\n",
      "Evaluation episode 5 finished: metric -479.285758, timedelta=0:00:00.351159\n",
      "Evaluation episode 6 finished: metric -465.367322, timedelta=0:00:00.527462\n",
      "Evaluation episode 7 finished: metric -469.887116, timedelta=0:00:00.355267\n",
      "Evaluation episode 8 finished: metric -474.777023, timedelta=0:00:00.256751\n",
      "Evaluation episode 9 finished: metric -481.679283, timedelta=0:00:00.235541\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reward</td>\n",
       "      <td>-472.928997</td>\n",
       "      <td>6.550939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coverage_prop</td>\n",
       "      <td>0.649381</td>\n",
       "      <td>0.180341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intensity_prop</td>\n",
       "      <td>0.396457</td>\n",
       "      <td>0.145935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.226063</td>\n",
       "      <td>0.098639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric        mean       std\n",
       "0          reward -472.928997  6.550939\n",
       "1   coverage_prop    0.649381  0.180341\n",
       "2  intensity_prop    0.396457  0.145935\n",
       "3              f1    0.226063  0.098639"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards, eval_res = debug_run(fname, max_peaks, params, n_eval_episodes=n_eval_episodes, deterministic=False)\n",
    "df = eval_res_to_df(rewards, eval_res)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load first re-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join('..', 'DQN', 'DQN_9_rerun_1.zip') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ionisation_mode': 'Positive', 'rt_range': (400, 500), 'isolation_window': 0.7, 'mz_tol': 10, 'rt_tol': 120, 'alpha': 0.191500954, 'beta': 0.030798858}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vimms-gym/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:561: UserWarning:\n",
      "\n",
      "This system does not have apparently enough memory to store the complete replay buffer 9.87GB > 0.34GB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation episode 0 finished: metric 125.119552, timedelta=0:00:02.810154\n",
      "Evaluation episode 1 finished: metric 147.436301, timedelta=0:00:03.308261\n",
      "Evaluation episode 2 finished: metric 143.023466, timedelta=0:00:01.739373\n",
      "Evaluation episode 3 finished: metric 142.359745, timedelta=0:00:03.149774\n",
      "Evaluation episode 4 finished: metric 150.115175, timedelta=0:00:04.013260\n",
      "Evaluation episode 5 finished: metric 160.712618, timedelta=0:00:02.833915\n",
      "Evaluation episode 6 finished: metric 119.778864, timedelta=0:00:02.997854\n",
      "Evaluation episode 7 finished: metric 144.988713, timedelta=0:00:02.988392\n",
      "Evaluation episode 8 finished: metric 113.195536, timedelta=0:00:03.152961\n",
      "Evaluation episode 9 finished: metric 145.361616, timedelta=0:00:02.212373\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reward</td>\n",
       "      <td>139.209159</td>\n",
       "      <td>14.133361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coverage_prop</td>\n",
       "      <td>0.976379</td>\n",
       "      <td>0.022579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intensity_prop</td>\n",
       "      <td>0.849789</td>\n",
       "      <td>0.030132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.521972</td>\n",
       "      <td>0.079312</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric        mean        std\n",
       "0          reward  139.209159  14.133361\n",
       "1   coverage_prop    0.976379   0.022579\n",
       "2  intensity_prop    0.849789   0.030132\n",
       "3              f1    0.521972   0.079312"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards, eval_res = debug_run(fname, max_peaks, params, n_eval_episodes=n_eval_episodes, deterministic=True)\n",
    "df = eval_res_to_df(rewards, eval_res)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ionisation_mode': 'Positive', 'rt_range': (400, 500), 'isolation_window': 0.7, 'mz_tol': 10, 'rt_tol': 120, 'alpha': 0.191500954, 'beta': 0.030798858}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vimms-gym/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:561: UserWarning:\n",
      "\n",
      "This system does not have apparently enough memory to store the complete replay buffer 9.87GB > 0.33GB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation episode 0 finished: metric 51.047035, timedelta=0:00:01.286196\n",
      "Evaluation episode 1 finished: metric 52.058043, timedelta=0:00:02.592784\n",
      "Evaluation episode 2 finished: metric 43.937356, timedelta=0:00:02.109299\n",
      "Evaluation episode 3 finished: metric 34.198545, timedelta=0:00:01.723674\n",
      "Evaluation episode 4 finished: metric 79.437551, timedelta=0:00:02.568079\n",
      "Evaluation episode 5 finished: metric 50.119367, timedelta=0:00:02.337249\n",
      "Evaluation episode 6 finished: metric 46.410382, timedelta=0:00:02.432139\n",
      "Evaluation episode 7 finished: metric 44.504599, timedelta=0:00:04.275583\n",
      "Evaluation episode 8 finished: metric 50.312273, timedelta=0:00:02.385027\n",
      "Evaluation episode 9 finished: metric 50.224249, timedelta=0:00:02.250241\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reward</td>\n",
       "      <td>50.224940</td>\n",
       "      <td>10.959397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coverage_prop</td>\n",
       "      <td>0.954365</td>\n",
       "      <td>0.033858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intensity_prop</td>\n",
       "      <td>0.828065</td>\n",
       "      <td>0.048414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.541052</td>\n",
       "      <td>0.106116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric       mean        std\n",
       "0          reward  50.224940  10.959397\n",
       "1   coverage_prop   0.954365   0.033858\n",
       "2  intensity_prop   0.828065   0.048414\n",
       "3              f1   0.541052   0.106116"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards, eval_res = debug_run(fname, max_peaks, params, n_eval_episodes=n_eval_episodes, deterministic=False)\n",
    "df = eval_res_to_df(rewards, eval_res)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load second re-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = os.path.join('..', 'DQN', 'DQN_9_rerun_2.zip') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vimms-gym/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:561: UserWarning:\n",
      "\n",
      "This system does not have apparently enough memory to store the complete replay buffer 9.87GB > 0.33GB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ionisation_mode': 'Positive', 'rt_range': (400, 500), 'isolation_window': 0.7, 'mz_tol': 10, 'rt_tol': 120, 'alpha': 0.191500954, 'beta': 0.030798858}\n",
      "Evaluation episode 0 finished: metric 132.219249, timedelta=0:00:04.016300\n",
      "Evaluation episode 1 finished: metric 134.872074, timedelta=0:00:02.738245\n",
      "Evaluation episode 2 finished: metric 130.427843, timedelta=0:00:02.071264\n",
      "Evaluation episode 3 finished: metric 126.086919, timedelta=0:00:03.131886\n",
      "Evaluation episode 4 finished: metric 136.770174, timedelta=0:00:02.624029\n",
      "Evaluation episode 5 finished: metric 116.921459, timedelta=0:00:02.083195\n",
      "Evaluation episode 6 finished: metric 124.990844, timedelta=0:00:03.497837\n",
      "Evaluation episode 7 finished: metric 109.730944, timedelta=0:00:02.914869\n",
      "Evaluation episode 8 finished: metric 119.027010, timedelta=0:00:01.894763\n",
      "Evaluation episode 9 finished: metric 108.694060, timedelta=0:00:01.202462\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reward</td>\n",
       "      <td>123.974058</td>\n",
       "      <td>9.529937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coverage_prop</td>\n",
       "      <td>0.939156</td>\n",
       "      <td>0.039377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intensity_prop</td>\n",
       "      <td>0.799242</td>\n",
       "      <td>0.058815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.466650</td>\n",
       "      <td>0.080420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric        mean       std\n",
       "0          reward  123.974058  9.529937\n",
       "1   coverage_prop    0.939156  0.039377\n",
       "2  intensity_prop    0.799242  0.058815\n",
       "3              f1    0.466650  0.080420"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards, eval_res = debug_run(fname, max_peaks, params, n_eval_episodes=n_eval_episodes, deterministic=True)\n",
    "df = eval_res_to_df(rewards, eval_res)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/vimms-gym/lib/python3.9/site-packages/stable_baselines3/common/buffers.py:561: UserWarning:\n",
      "\n",
      "This system does not have apparently enough memory to store the complete replay buffer 9.87GB > 0.32GB\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ionisation_mode': 'Positive', 'rt_range': (400, 500), 'isolation_window': 0.7, 'mz_tol': 10, 'rt_tol': 120, 'alpha': 0.191500954, 'beta': 0.030798858}\n",
      "Evaluation episode 0 finished: metric 31.888336, timedelta=0:00:01.581181\n",
      "Evaluation episode 1 finished: metric 23.143872, timedelta=0:00:01.509766\n",
      "Evaluation episode 2 finished: metric 36.427806, timedelta=0:00:02.155045\n",
      "Evaluation episode 3 finished: metric 36.680873, timedelta=0:00:01.426064\n",
      "Evaluation episode 4 finished: metric 14.809011, timedelta=0:00:01.454375\n",
      "Evaluation episode 5 finished: metric 25.413376, timedelta=0:00:01.590618\n",
      "Evaluation episode 6 finished: metric 53.616078, timedelta=0:00:02.351283\n",
      "Evaluation episode 7 finished: metric 50.523153, timedelta=0:00:02.262978\n",
      "Evaluation episode 8 finished: metric 50.846056, timedelta=0:00:02.531238\n",
      "Evaluation episode 9 finished: metric 50.139359, timedelta=0:00:02.055096\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reward</td>\n",
       "      <td>37.348792</td>\n",
       "      <td>12.912589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coverage_prop</td>\n",
       "      <td>0.952008</td>\n",
       "      <td>0.041648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>intensity_prop</td>\n",
       "      <td>0.795199</td>\n",
       "      <td>0.062697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f1</td>\n",
       "      <td>0.490124</td>\n",
       "      <td>0.067185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           metric       mean        std\n",
       "0          reward  37.348792  12.912589\n",
       "1   coverage_prop   0.952008   0.041648\n",
       "2  intensity_prop   0.795199   0.062697\n",
       "3              f1   0.490124   0.067185"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards, eval_res = debug_run(fname, max_peaks, params, n_eval_episodes=n_eval_episodes, deterministic=False)\n",
    "df = eval_res_to_df(rewards, eval_res)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7791ce17e478d6b0fb6b31f66d51580be045b7d8633e3f5b85f8af4f319f0917"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
